{"cells":[{"cell_type":"code","source":["#import statements here\nfrom pyspark.ml import feature\nfrom pyspark.ml import clustering\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql import functions as fn\nimport numpy as np\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml import feature, regression, evaluation, Pipeline\nfrom pyspark.sql import functions as fn, Row\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom pyspark.sql import functions as sf\nfrom pyspark.ml.feature import CountVectorizer\nfrom pyspark.ml.feature import IDF\nfrom pyspark.ml.feature import RegexTokenizer\nimport requests\nfrom pyspark.ml.feature import StopWordsRemover\nfrom pyspark.sql.functions import concat, col, lit, lower\nfrom pyspark.sql.functions import isnan, when, count, col, isnull\nfrom pyspark.sql.functions import concat_ws\nfrom  pyspark.sql.functions import abs\n# seting master(\"local[*]\") enables multicore processing on all available logical cores on your machine\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\nfrom pyspark.ml.classification import LogisticRegression, RandomForestClassifier\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.tuning import CrossValidatorModel\nfrom pyspark.ml import evaluation\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import evaluation\nfrom pyspark.ml.classification import LogisticRegression, RandomForestClassifier\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["import os\n\n# Define a function to determine if we are running on data bricks\n# Return true if running in the data bricks environment, false otherwise\ndef is_databricks():\n    # get the databricks runtime version\n    db_env = os.getenv(\"DATABRICKS_RUNTIME_VERSION\")\n    \n    # if running on data bricks\n    if db_env != None:\n        return True\n    else:\n        return False\n\n# Define a function to read the data file.  The full path data file name is constructed\n# by checking runtime environment variables to determine if the runtime environment is \n# databricks, or a student's personal computer.  The full path file name is then\n# constructed based on the runtime env.\n# \n# Params\n#   data_file_name: The base name of the data file to load\n# \n# Returns the full path file name based on the runtime env\n#\ndef get_training_filename(data_file_name):    \n    # if running on data bricks\n    if is_databricks():\n        # build the full path file name assuming data brick env\n        full_path_name = \"/FileStore/tables/%s\" % data_file_name\n    # else the data is assumed to be in the same dir as this notebook\n    else:\n        # Assume the student is running on their own computer and load the data\n        # file from the same dir as this notebook\n        full_path_name = data_file_name\n    \n    # return the full path file name to the caller\n    return full_path_name"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["#load the files\nairlines_df = spark.read.csv(get_training_filename('airlines.csv'), header=True, inferSchema=True)\nairports_df = spark.read.csv(get_training_filename('airports.csv'), header=True, inferSchema=True)\nflights_df = spark.read.csv(get_training_filename('flights.csv'), header=True, inferSchema=True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["#EDA\n\nshape = ((flights_df.count(), len(flights_df.columns)))  #prints the shape\nprint('The shape of flights_df:', shape)\n\nprint(flights_df.select([count(when(isnull(c), c)).alias(c) for c in flights_df.columns])) # prints the null values in the columns\n\nprint(flights_df.printSchema())  #shows the schema of the datasset\n\nflights_df_sample = flights_df.sample(True, 0.5, 42)\nflights_df_sample_pandas = flights_df_sample.toPandas() #create sample for being able to create proper visualizations on the dataset\n\n#create visualizations for relevant features to understand the data\nsns.distplot(flights_df_sample_pandas['MONTH'])\ndisplay()\n\nsns.distplot(flights_df_sample_pandas['DAY'])\ndisplay()\n\n\nsns.distplot(flights_df_sample_pandas['DAY_OF_WEEK'])\ndisplay()\n\n\nflights_df_sample_pandas['AIRLINE'].value_counts().plot(kind='bar')\ndisplay()\n\nairline_count  = flights_df_sample_pandas['AIRLINE'].value_counts()\nairline_count = airline_count[:10,]\nplt.figure(figsize=(10,5))\nsns.barplot(airline_count.index, airline_count.values, alpha=0.8)\ndisplay()\n\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["#create visualizations for relevant features to understand the data\noa_count  = flights_df_sample_pandas['ORIGIN_AIRPORT'].value_counts()\noa_count = oa_count[:10,]\nplt.figure(figsize=(10,5))\nsns.barplot(oa_count.index, oa_count.values, alpha=0.8)\ndisplay()\n\nda_count  = flights_df_sample_pandas['DESTINATION_AIRPORT'].value_counts()\nda_count = da_count[:10,]\nplt.figure(figsize=(10,5))\nsns.barplot(da_count.index, da_count.values, alpha=0.8)\ndisplay()\n\n\nsns.distplot(flights_df_sample_pandas['SCHEDULED_DEPARTURE'])\ndisplay()\n\n\nsns.distplot(flights_df_sample_pandas['DEPARTURE_DELAY'])\ndisplay()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["#create visualizations for relevant features to understand the data\nsns.distplot(flights_df_sample_pandas['DISTANCE'])\ndisplay()\n\nsns.distplot(flights_df_sample_pandas['SCHEDULED_ARRIVAL'])\ndisplay()\n\nsns.barplot(x=\"MONTH\", y=\"DEPARTURE_DELAY\", data=flights_df_sample_pandas)\ndisplay()\n\n\ndf3 = flights_df.groupBy('MONTH').avg('DEPARTURE_DELAY').toPandas()\ndf3\nfig4 = sns.barplot(x = 'MONTH', y = 'avg(DEPARTURE_DELAY)', data = df3)\ndisplay()\n\nsns.barplot(x=\"AIRLINE\", y=\"DEPARTURE_DELAY\", hue=\"Flight_Distance\", data=flights_df_sample_pandas)\ndisplay()\n\nsns.barplot(x=\"SCHEDULED_DEPARTURE\", y=\"DEPARTURE_DELAY\", data=flights_df_sample_pandas)\ndisplay()\n\n\nsns.barplot(x=\"ORIGIN_AIRPORT\", y=\"DEPARTURE_DELAY\", data=flights_df_sample_pandas)\ndisplay()\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["#create visualizations for relevant features to understand the data\nsns.barplot(x=\"AIRLINE\", y=\"DEPARTURE_DELAY\", data=flights_df_sample_pandas)\ndisplay()\n\n\nsns.barplot(x=\"DAY_OF_WEEK\", y=\"DEPARTURE_DELAY\", hue=\"Flight_Distance\", data=flights_df_sample_pandas)\ndisplay()\n\n\nsns.barplot(x=\"DAY_OF_WEEK\", y=\"DEPARTURE_DELAY\", data=flights_df_sample_pandas)\ndisplay()\n\nsns.barplot(x=\"DAY\", y=\"DEPARTURE_DELAY\", hue=\"Flight_Distance\", data=flights_df_sample_pandas)\ndisplay()\n\nsns.barplot(x=\"DAY\", y=\"DEPARTURE_DELAY\", data=flights_df_sample_pandas)\ndisplay()\n\nsns.barplot(x=\"MONTH\", y=\"DEPARTURE_DELAY\", hue=\"Flight_Distance\", data=flights_df_sample_pandas)\ndisplay()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#DATA PREPARATION: now we will process the dataset and convert into a usable dataset\n\n#create dataframe with columns we will consider\nflights_df = flights_df.select('MONTH', 'DAY', 'DAY_OF_WEEK', 'AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'SCHEDULED_DEPARTURE', 'DEPARTURE_DELAY', 'DISTANCE', 'SCHEDULED_ARRIVAL', 'ARRIVAL_DELAY', 'CANCELLED')  \n\n#remove columns with cancel = 0 \nflights_df = flights_df.filter((fn.col('CANCELLED')==0))\n\n#set up a threshold for the delay time\nflights_df = flights_df.withColumn(\"Flight_Delayed\", fn.when(fn.col(\"DEPARTURE_DELAY\")<15, 0).otherwise(1))\n\n#create buckets for distance as long, short and medium\nfrom pyspark.ml.feature import Bucketizer\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import *\nbucketizer = Bucketizer(splits=[ 0, 100, 1000, float('Inf') ],inputCol=\"DISTANCE\", outputCol=\"Distance_Bucket\")\nflights_df = bucketizer.setHandleInvalid(\"keep\").transform(flights_df)\nt = {0.0:\"Short\", 1.0: \"Medium\", 2.0:\"Long\"}\nudf_foo = udf(lambda x: t[x], StringType())\nflights_df = flights_df.withColumn(\"Flight_Distance\", udf_foo(\"Distance_Bucket\"))\n\n#perform string index, which prevents creation of new columns for the dummy variables and perform one hot encoding\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import OneHotEncoder\n\nindexer = StringIndexer(inputCol=\"AIRLINE\", outputCol=\"Airline_Numeric\").fit(flights_df)\nflights_df = indexer.transform(flights_df)\n\n\nencoder = OneHotEncoder(inputCol=\"Airline_Numeric\", outputCol=\"Airline_OHE\")\nflights_df= encoder.transform(flights_df)\n\nindexer = StringIndexer(inputCol=\"ORIGIN_AIRPORT\", outputCol=\"OA_Numeric\").fit(flights_df)\nflights_df = indexer.transform(flights_df)\n\nencoder = OneHotEncoder(inputCol=\"OA_Numeric\", outputCol=\"Origin_Airport_OHE\")\nflights_df= encoder.transform(flights_df)\n\nindexer = StringIndexer(inputCol=\"DESTINATION_AIRPORT\", outputCol=\"DA_Numeric\").fit(flights_df)\nflights_df = indexer.transform(flights_df)\n\nencoder = OneHotEncoder(inputCol=\"DA_Numeric\", outputCol=\"Destination_Airport_OHE\")\nflights_df= encoder.transform(flights_df)\n\nflights_df = flights_df.drop('ARRIVAL_DELAY')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["#that dataset is imbalanced\n\n#perform undersampling for the preparation of the data\n#split the train test data\ntraining_df, testing_df = flights_df.randomSplit([0.9, 0.1], seed=5)\n\n#create major and minor DF \nmajor_df = training_df.filter(col(\"Flight_Delayed\") == 0)\nminor_df = training_df.filter(col(\"Flight_Delayed\") == 1)\nratio = major_df.count()/minor_df.count()\n\n#create the balanced dataset for training the model\nsampled_majority_df = major_df.sample(False, 1.2/ratio, seed=5)\ncombined_df_2 = sampled_majority_df.unionAll(minor_df)\n\n#shows the count of balanced dataframe\ncombined_df_2.groupBy('Flight_Delayed').agg(fn.count('*')).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+--------+\nCANCELLED|count(1)|\n+---------+--------+\n        0| 5729195|\n+---------+--------+\n\n+--------------+--------+\nFlight_Delayed|count(1)|\n+--------------+--------+\n             1|  949914|\n             0| 1139995|\n+--------------+--------+\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["#implementation of prepared and undersampled dataset for the model implementation\n\n#create a vector assembler\nva = VectorAssembler(\n    inputCols=[\"MONTH\", \"DAY\", \"DAY_OF_WEEK\", \"Airline_OHE\", \"Origin_Airport_OHE\", \"Destination_Airport_OHE\", \"SCHEDULED_DEPARTURE\", \"Distance_Bucket\", \"SCHEDULED_ARRIVAL\", \"CANCELLED\"], outputCol=\"features\")\n\n#train a logistic model\nlr = LogisticRegression(featuresCol='features', labelCol='Flight_Delayed', regParam=0.2, elasticNetParam=0.1, threshold=0.45, thresholds=[0.55, 0.45])\nlr_pipeline = Pipeline(stages=[va, lr])\nlr_model = lr_pipeline.fit(combined_df_2)\nlr_transform = lr_model.transform(testing_df)\n\n#logistic model metric evaluation\n\n#AUC score\nbce = BinaryClassificationEvaluator(labelCol='Flight_Delayed', metricName='areaUnderROC')\nscore_auc=bce.evaluate(lr_transform)\n\n#Recall\nlr_evaluator_recall = evaluation.MulticlassClassificationEvaluator(labelCol=\"Flight_Delayed\", metricName=\"weightedRecall\")\nscore_recall=lr_evaluator_recall.evaluate(lr_model.transform(testing_df))\n\n#Precision\nlr_evaluator_precision = evaluation.MulticlassClassificationEvaluator(labelCol=\"Flight_Delayed\", metricName=\"weightedPrecision\")\nscore_precision=lr_evaluator_precision.evaluate(lr_model.transform(testing_df))\n\n#accuracy\nlr_evaluator_accuracy = evaluation.MulticlassClassificationEvaluator(labelCol=\"Flight_Delayed\", metricName=\"accuracy\")\nscore_accuracy=lr_evaluator_accuracy.evaluate(lr_model.transform(testing_df))\n\n#f1 \nlr_evaluator_accuracy = evaluation.MulticlassClassificationEvaluator(labelCol=\"Flight_Delayed\", metricName=\"f1\")\nscore_f1=lr_evaluator_accuracy.evaluate(lr_model.transform(testing_df))\n\n#shows the scatterplot\nplt.figure(figsize=(10,6))\nplt.plot([0, 1], [0, 1], 'r--')\nplt.scatter(lr_model.stages[-1].summary.roc.select('FPR').collect(),\n            lr_model.stages[-1].summary.roc.select('TPR').collect())\nplt.title('ROC Scatter Plot : TPR/FPR')\nplt.xlabel('FPR')\nplt.ylabel('TPR')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["#\nscores = [score_auc, score_recall, score_accuracy, score_f1, score_precision]\nmetricName = ['AUC', 'Recall', 'Accuracy', 'f1', 'Precision']\nmetric_df = pd.DataFrame(zip(metricName,scores),index=[1,2,3,4,5],columns=['Metric Name', 'Score'])\nmetric_df.head(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Metric Name</th>\n      <th>Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>AUC</td>\n      <td>0.632220</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Recall</td>\n      <td>0.569218</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Accuracy</td>\n      <td>0.569218</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>f1</td>\n      <td>0.617380</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Precision</td>\n      <td>0.757864</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["#Confusion matrix of logistic regression\n\ny_true = lr_transform.select(['Flight_Delayed']).collect()\ny_pred = lr_transform.select(['prediction']).collect()\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_true, y_pred))\nconfusion_matrix(y_true, y_pred)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">              precision    recall  f1-score   support\n\n           0       0.87      0.55      0.68    467809\n           1       0.25      0.65      0.36    105586\n\n   micro avg       0.57      0.57      0.57    573395\n   macro avg       0.56      0.60      0.52    573395\nweighted avg       0.76      0.57      0.62    573395\n\nOut[23]: array([[258188, 209621],\n       [ 37387,  68199]])</div>"]}}],"execution_count":12},{"cell_type":"code","source":["#train the GBT model on the balanced dataset\ngbt = GBTClassifier(featuresCol='features', labelCol='Flight_Delayed', maxIter=5, maxBins=15, stepSize=0.08, maxDepth=4)\ngbt_pipeline = Pipeline(stages=[va, gbt])\ngbt_model = gbt_pipeline.fit(combined_df_2)\ngbt_transform = gbt_model.transform(testing_df)\n\n#evaluation metric for GBT\ngbt_bce = BinaryClassificationEvaluator(labelCol='Flight_Delayed', metricName='areaUnderROC')\nscore_auc=gbt_bce.evaluate(gbt_transform)\n\n#f1\ngbt_evaluator_f1 = evaluation.MulticlassClassificationEvaluator(labelCol=\"Flight_Delayed\", metricName=\"f1\")\nscore_f1=gbt_evaluator_f1.evaluate(gbt_transform)\n\n#accuracy\ngbt_evaluator_accuracy = evaluation.MulticlassClassificationEvaluator(labelCol=\"Flight_Delayed\", metricName=\"accuracy\")\nscore_accuracy=gbt_evaluator_accuracy.evaluate(gbt_transform)\n\n#precision\ngbt_evaluator_precision = evaluation.MulticlassClassificationEvaluator(labelCol=\"Flight_Delayed\", metricName=\"weightedPrecision\")\nscore_precision=gbt_evaluator_precision.evaluate(gbt_transform)\n\n#recall\ngbt_evaluator_recall = evaluation.MulticlassClassificationEvaluator(labelCol=\"Flight_Delayed\", metricName=\"weightedRecall\")\nscore_recall=gbt_evaluator_recall.evaluate(gbt_transform)\n\n#dataframe for the evaluation metric\nscores = [score_auc, score_recall, score_accuracy, score_f1, score_precision]\nmetricName = ['AUC', 'Recall', 'Accuracy', 'f1', 'Precision']\n\nmetric_df = pd.DataFrame(zip(metricName,scores),index=[1,2,3,4,5],columns=['Metric Name', 'Score'])\nmetric_df"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Metric Name</th>\n      <th>Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>AUC</td>\n      <td>0.658491</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Recall</td>\n      <td>0.661530</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Accuracy</td>\n      <td>0.661530</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>f1</td>\n      <td>0.694910</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Precision</td>\n      <td>0.758869</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["#Confusion matrix of logistic regression\n\ny_true = gbt_transform.select(['Flight_Delayed']).collect()\ny_pred = gbt_transform.select(['prediction']).collect()\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_true, y_pred))\nconfusion_matrix(y_true, y_pred)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">              precision    recall  f1-score   support\n\n           0       0.87      0.69      0.77    467809\n           1       0.28      0.53      0.37    105586\n\n   micro avg       0.66      0.66      0.66    573395\n   macro avg       0.57      0.61      0.57    573395\nweighted avg       0.76      0.66      0.69    573395\n\nOut[28]: array([[323286, 144523],\n       [ 49554,  56032]])</div>"]}}],"execution_count":14},{"cell_type":"code","source":["#create a further sample due to computation limit\nsample = combined_df_2.sample(True, 0.2,seed=5)\n\n#uncomment below comment if the error says label exists and run\n\n# flights_df = flights_df.drop('label')\n# training_df = training_df.drop('label')\n# testing_df = testing_df.drop('label')\n# sample = sample.drop('label')\n\nfeatures = sample.columns\nsample = sample.select(col(\"Flight_Delayed\").alias(\"label\"), *features)\n\n#perform train test to get the testing dataset\ntraining_df, testing_df = flights_df.randomSplit([0.9, 0.1], seed=5)\n\n#create vectorassembler \nva = VectorAssembler(inputCols=[\"MONTH\", \"DAY\", \"DAY_OF_WEEK\", \"Airline_OHE\", \"Origin_Airport_OHE\", \"Destination_Airport_OHE\", \"SCHEDULED_DEPARTURE\", \"Distance_Bucket\", \"SCHEDULED_ARRIVAL\", \"CANCELLED\"], outputCol=\"unscaled_features\")\n\n#standardize the input\nstandardScaler = StandardScaler(inputCol=\"unscaled_features\", outputCol=\"features\")\n\n#train model and create pipeline with crossvalidation\nrf = RandomForestClassifier(featuresCol='features', labelCol='label')\nrf_pipeline = Pipeline(stages=[va,standardScaler, rf])\n\n\nparamGrid = ParamGridBuilder() \\\n    .addGrid(rf.maxDepth, [3,4]) \\\n    .addGrid(rf.numTrees, [100,150]) \\\n    .addGrid(rf.impurity,[\"Gini\"]) \\\n    .addGrid(rf.featureSubsetStrategy,[\"auto\", \"sqrt\"]) \\\n    .addGrid(rf.maxBins,[100]) \\\n    .build()\ncrossval_rf = CrossValidator(estimator=rf_pipeline,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=BinaryClassificationEvaluator(),\n                          numFolds=3)\n\ncvModel = crossval_rf.fit(sample)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/spark/python/pyspark/ml/util.py:791: UserWarning: Can not find mlflow. To enable mlflow logging, install MLflow library from PyPi.\n  warnings.warn(_MLflowInstrumentation._NO_MLFLOW_WARNING)\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["#evaluation metric for random forest\ntesting_df = testing_df.select(col(\"Flight_Delayed\").alias(\"label\"), *features)\nprediction = cvModel.transform(testing_df)\n\n#AUC evaluation\nbce = BinaryClassificationEvaluator(labelCol='label')\nscore_auc = bce.evaluate(prediction, {bce.metricName: \"areaUnderROC\"} )\n\n#weighted precision\nmce = evaluation.MulticlassClassificationEvaluator(labelCol=\"Flight_Delayed\", metricName=\"weightedPrecision\")\nscore_precision = mce.evaluate(cvModel.transform(testing_df))\n\n#weighted Recall\nmce = evaluation.MulticlassClassificationEvaluator(labelCol=\"Flight_Delayed\", metricName=\"weightedRecall\")\nscore_recall = mce.evaluate(cvModel.transform(testing_df))\n\n#Accuracy\nmce = evaluation.MulticlassClassificationEvaluator(labelCol=\"Flight_Delayed\", metricName=\"accuracy\")\nscore_accuracy = mce.evaluate(cvModel.transform(testing_df))\n\n#f1\nmce = evaluation.MulticlassClassificationEvaluator(labelCol=\"Flight_Delayed\", metricName=\"f1\")\nscore_f1= mce.evaluate(cvModel.transform(testing_df))\n\n#dataframe for the evaluation metric\nscores = [score_auc, score_recall, score_accuracy, score_f1, score_precision]\nmetricName = ['AUC', 'Recall', 'Accuracy', 'f1', 'Precision']\n\nmetric_df = pd.DataFrame(zip(metricName,scores),index=[1,2,3,4,5],columns=['Metric Name', 'Score'])\nmetric_df"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Metric Name</th>\n      <th>Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>AUC</td>\n      <td>0.650553</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Recall</td>\n      <td>0.815858</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Accuracy</td>\n      <td>0.815858</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>f1</td>\n      <td>0.733124</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Precision</td>\n      <td>0.665625</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":17}],"metadata":{"name":"EDA+DELAY","notebookId":3013050802831362},"nbformat":4,"nbformat_minor":0}
